{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting working directory\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/Users/akailwoo/Desktop/Ounass/DS/20190706_DS_ppt/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Social_Network_Ads.csv')\n",
    "x = df.iloc[:, 2:4]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test,y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akailwoo/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/akailwoo/anaconda3/lib/python3.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Users/akailwoo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#feature scaling\n",
    "# options available: Normalizer()  MinMaxScaler()  RobustScaler()  StandardScaler()\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "sc_x = preprocessing.StandardScaler()\n",
    "x_train = sc_x.fit_transform(x_train)\n",
    "x_test = sc_x.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 500, criterion = 'gini', max_depth = 7, random_state =0, verbose = 1)\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestClassifier in module sklearn.ensemble.forest:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  RandomForestClassifier(n_estimators='warn', criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
      " |  \n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is always the same as the original\n",
      " |  input sample size but the samples are drawn with replacement if\n",
      " |  `bootstrap=True` (default).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : integer, optional (default=10)\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |         The default value of ``n_estimators`` will change from 10 in\n",
      " |         version 0.20 to 100 in version 0.22.\n",
      " |  \n",
      " |  criterion : string, optional (default=\"gini\")\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : integer or None, optional (default=None)\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int, float, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int, float, optional (default=1)\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, optional (default=0.)\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, (default=1e-7)\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  \n",
      " |  bootstrap : boolean, optional (default=True)\n",
      " |      Whether bootstrap samples are used when building trees.\n",
      " |  \n",
      " |  oob_score : bool (default=False)\n",
      " |      Whether to use out-of-bag samples to estimate\n",
      " |      the generalization accuracy.\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, optional (default=False)\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  class_weight : dict, list of dicts, \"balanced\", \"balanced_subsample\" or     None, optional (default=None)\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      " |      weights are computed based on the bootstrap sample for every tree\n",
      " |      grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : array of shape = [n_classes] or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : array of shape = [n_features]\n",
      " |      The feature importances (the higher, the more important the feature).\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |  \n",
      " |  oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  \n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
      " |  ...                              random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      " |              max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
      " |              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      " |              min_samples_leaf=1, min_samples_split=2,\n",
      " |              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      " |              oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      " |  >>> print(clf.feature_importances_)\n",
      " |  [0.14205973 0.76664038 0.0282433  0.06305659]\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  DecisionTreeClassifier, ExtraTreesClassifier\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      abc.NewBase\n",
      " |      BaseForest\n",
      " |      abc.NewBase\n",
      " |      sklearn.ensemble.base.BaseEnsemble\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators='warn', criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest. The\n",
      " |      class probability of a single tree is the fraction of samples of the same\n",
      " |      class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      " |          Return a node indicator matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |      \n",
      " |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Returns iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[65  3]\n",
      " [ 3 29]]\n"
     ]
    }
   ],
   "source": [
    "#making the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm= confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   12.1s finished\n",
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n",
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualizing results\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "x_set, y_set = x_train, y_train\n",
    "x1, x2 = np.meshgrid(np.arange(start = x_set[:,0].min() -1, stop = x_set[:,0].max() +1 , step = 0.01),\n",
    "                     np.arange(start = x_set[:,1].min() -1, stop = x_set[:,1].max() +1 , step = 0.01))\n",
    "\n",
    "plt.contourf(x1, x2, classifier.predict(np.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "\n",
    "plt.xlim(x1.min(), x1.max())\n",
    "plt.ylim(x2.min(), x2.max())\n",
    "\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(x_set[y_set == j, 0], x_set[y_set==j, 1],\n",
    "               c = ListedColormap(('pink', 'blue'))(i), label = j)\n",
    "#     print(i)\n",
    "#     print(j)\n",
    "plt.title('Random Forests')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
